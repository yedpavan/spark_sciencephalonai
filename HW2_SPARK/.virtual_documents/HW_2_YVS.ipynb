








#import libraries
import pyspark
import random
import warnings
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, IntegerType, StringType,ArrayType
from pyspark.sql.functions import col,lit
import pandas as pd
import time
from pyspark import SparkContext
from IPython.display import Image
from IPython.core.display import HTML 
import fastavro





#read the csv file using spark
df = spark.read.csv("/databricks-datasets/airlines/part-00000", header=True, inferSchema=True)
df.display()

#Header = True -> treats the first row as column names
#inferSchema = True -> ensures that Spark automatically detects and assigns the correct data types for each column





#if tmp folder already exists, delete it
dbutils.fs.rm("/tmp/", recurse=True)



#save the dataframe in different formats in different folders
#method 1:
format_list = ['csv','json','parquet','orc','avro','delta']
for format_ in format_list:
    df.write.format(format_).save('/tmp/airlines_'+format_)






#display the folders in tmp folder
display(dbutils.fs.ls("/tmp/"))


#display the files in airlines_parquet folder
display(dbutils.fs.ls("/tmp/airlines_parquet"))


#if tmp2 folder already exists, delete it
dbutils.fs.rm("/tmp2/", recurse=True)


# method 2: saving as a single file in different folders
format_list = ['csv','json','parquet','orc','avro','delta']
for format_ in format_list:
    df.coalesce(1).write.format(format_).save("/tmp2/airlines_file_"+format_)


#display contents in temp2 folder
display(dbutils.fs.ls("/tmp2/"))


#display contents in temp2/airlines_single_file_parquet folder
display(dbutils.fs.ls("/tmp2/airlines_file_parquet"))














#if tmp3 folder already exists, delete it
dbutils.fs.rm("/tmp3/", recurse=True)
#if tmp3 folder already exists, delete it
dbutils.fs.rm("/tmp2_copy/", recurse=True)


#place holder folder to copy the files from temp2 folder
source_folder = "/tmp2/"
destination_folder = "/tmp2_copy/"

# List all files in the source folder
files = dbutils.fs.ls(source_folder)

# Iterate over each file and copy it to the destination folder
for file in files:
    src_file_path = file.path
    dst_file_path = destination_folder + file.name
    dbutils.fs.cp(src_file_path, dst_file_path, recurse=True)

# Verify the files in the destination folder
display(dbutils.fs.ls(destination_folder))


#code to access the single files in all the folders and move to a new folder


folders = dbutils.fs.ls("/tmp2_copy/")
for i in range(len(folders)):
    folder_name = folders[i].name
    #print(folder_name)
    files = dbutils.fs.ls("/tmp2_copy/"+folder_name)
    for file in files: 
        if file.name.startswith("part-"):
            dbutils.fs.mv(file.path, "/tmp3/"+file.name)




display(dbutils.fs.ls("/tmp3/"))
#display(dbutils.fs.ls("/tmp3/airlines_file_avro/"))





#display the delta folder
display(dbutils.fs.ls("/tmp2/airlines_file_delta"))


files_tmp3 = dbutils.fs.ls("/tmp3/")

# Iterate over folders and print their sizes
for file in files_tmp3:

    size_mb = file.size / (1024 * 1024)  # Convert size to MB
    print(f"File: {file.name}, Size: {size_mb:.2f} MB")





#Read all the files and measure the read times:
for file in files_tmp3:
    start_time = time.time() 
    df_new = spark.read.csv("/tmp3/"+file.name)
    end_time = time.time()
    print(f"Time taken to read :{file.name} {end_time - start_time:.4f} seconds")











# read csv without specifying schema

df_inferred = spark.read.format("csv").option("header", "true").option("inferSchema", "true").load("/FileStore/tables/data3.csv")
df_inferred.printSchema()



df_inferred.display()


# read csv with specified schema

explicit_schema = StructType([
    StructField("Id", IntegerType(), True),
    StructField("Name", StringType(), True),
    StructField("Age", IntegerType(), True),
    StructField("Experience", IntegerType(), True),
    StructField("Salary", IntegerType(), True)
])
df_explicit = spark.read.format("csv").option("header", "true").schema(explicit_schema).load("/FileStore/tables/data3.csv")
df_explicit.printSchema()








# read a dataframe to avro format avro 
df_inferred.write.format("avro").mode("overwrite").save("/tmp4/sample_avro")


files_avro = dbutils.fs.ls("/tmp4/sample_avro")
for file in files_avro:
    print(file.path)


#identify the file path of avro file
files_avro = dbutils.fs.ls("/tmp4/sample_avro")
for file in files_avro:
    if file.name.startswith("part-"):
        avro_file_path = file.path
        print(avro_file_path)



display(dbutils.fs.ls("/tmp4/sample_avro"))








# Modify the schema
# we can add a new column "City" to the DataFrame
df_modified = df_inferred.withColumn("City", lit("Unknown"))

# Show the modified DataFrame schema
df_modified.printSchema()

df_modified.write.format("avro").mode("overwrite").save("/tmp4/sample_avro_modified")


# Read the original Avro data
df_original = spark.read.format("avro").load("/tmp4/sample_avro")

# Read the modified Avro data
df_modified_read = spark.read.format("avro").load("/tmp4/sample_avro_modified")

# Show the schemas and data
df_original.printSchema()
df_original.show()

df_modified_read.printSchema()
df_modified_read.show()








#write the dataframe as a Delta Table
df_inferred.write.format("delta").mode("overwrite").save("/tmp4/sample_delta")


#modify the Schema

df_org_delta = spark.read.format("delta").load("/tmp4/sample_delta")





df_org_delta.display()


















